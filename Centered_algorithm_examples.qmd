---
title: "Some examples of centered algorithm"
author: "Juan Li"
date: 2025-06-23
format: 
  html:
    toc: true
    html-math-method: katex
    css: styles.css
editor: visual
---

```{r}
#| warning: false
#| message: false
#| output: false

library(dplyr)       # Data manipulation
library(ggplot2)     # Data visualization
library(ggpubr)      # ggarrange 
library(car)         # sample data
library(fastDummies) # make dummy variables
library(rms)
library(rootSolve)   # Nonlinear Root Finding, Equilibrium and Steady-State Analysis of Ordinary Differential Equations
```

We will use the `car::Prestige` dataset for the examples.

```{r}
help("Prestige")
head(Prestige)

# for simplicity, remove `NA`s
data <- Prestige[complete.cases(Prestige), ]
```

# One predictor

## Centered algorithm with a *dichotomous* predictor

Equation of a simple linear regression model: $y = \beta_0 + \beta_1x,\ x=0\ or\ 1$. Here, $\beta_0$ is the mean of $y$ when $x=0$, and $\beta_0 + \beta_1$ is the mean of $y$ when $x=1$. Now, change this into a centered algorithm, and the mean of $x$ is $\overline{x} = \frac{n_{x=1}}{n_{x=0}+n_{x=1}}$, we have

$$
\begin{split}
y &= \beta_0 + \beta_1x \\
&= \beta_0 + \beta_1(x-\overline{x}+\overline{x}) \\
&= (\beta_0 + \beta_1\overline{x}) + \beta_1(x-\overline{x})
\end{split}
$$

If set $\beta_{0,new} = \beta_0 + \beta_1\overline{x}$, then we have: $y = \beta_{0,new} + \beta_1(x-\overline{x})$

Here, $\beta_{0,new}$ should be the mean of $y$ within the entire cohort, $\overline{y}$, let’s check:

$$
\begin{split}
\beta_{0,new} 
&= \beta_0+\beta_1\overline{x} \\
&= \beta_0+\beta_1\frac{n_{x=1}}{n_{x=0}+n_{x=1}} \\
&= \frac{\beta_0(n_{x=0}+n_{x=1}) + \beta_1n_{x=1}}{n_{x=0}+n_{x=1}} \\
&= \frac{\beta_0n_{x=0} + (\beta_0 + \beta_1)n_{x=1}}{n_{x=0}+n_{x=1}} \\&= \overline{y}
\end{split}
$$

**Therefore, the centered algorithm can be written as** $y = \overline{y} + \beta_1(x-\overline{x})$.

```{r}
#| warning: false
#| message: false

# create a binary variable
data <- data %>% 
  mutate(higher_education = ifelse(education >= 12, 1, 0)) 
prop.table(table(data$higher_education))

# centering
(x_mean <- mean(data$higher_education))
data <- data %>% 
  mutate(higher_education_C = higher_education - x_mean) 
head(data %>% select(education, higher_education, higher_education_C))

# --- linear regression model on `higher_education` ---
paste0("Mean prestige score if `higher_education == 0`: ", mean(data$prestige[which(data$higher_education == 0)]))
paste0("Mean prestige score if `higher_education == 1`: ", mean(data$prestige[which(data$higher_education == 1)]))
fit <- lm(prestige ~ higher_education, data = data)
coef(fit)
sum(coef(fit)) # beta_0 + beta_1

# --- linear regression model on centered variable `higher_education_C` ---"
paste0("Mean prestige score in the dataset: ", mean(data$prestige))
fit <- lm(prestige ~ higher_education_C, data = data)
coef(fit)
```

## Centered algorithm with a *categorical* predictor

First, the categorical variable needs to be transformed into dummy variables. A categorical variable $x$ with $m$ levels: $cat_1$, $cat_2$,..., $cat_m$, can be transformed into $m-1$ dummy variables. Suppose $m=3$, then we have:

$$
x_{cat_2} =
\begin{cases}
1 ,& x = cat_2 \\
0  ,& otherwise
\end{cases}       
$$

and

$$
x_{cat_3} =
\begin{cases}
1 ,& x = cat_3 \\
0  ,& otherwise
\end{cases}       
$$

Then, when $x = cat_1$, $x_{cat_2} = x_{cat_3} = 0$.

Equation of a simple linear regression model: $y = \beta_0 + \beta_{cat_2}x_{cat_2}+ \beta_{cat_3}x_{cat_3}$. Here, $\beta_0$ is the mean of $y$ when $x = cat_1$, $\beta_0 + \beta_{cat_2}$ is the mean of $y$ when $x=cat_2$, and $\beta_0 + \beta_{cat_3}$ is the mean of $y$ when $x=cat_3$. Now, change this into a centered algorithm:

$$
\begin{split}
y &= \beta_0 + \beta_{cat_2}x_{cat_2} + \beta_{cat_3}x_{cat_3} \\
&= \beta_0 + \beta_{cat_2}(x_{cat_2} - \overline{x_{cat_2}} + \overline{x_{cat_2}})  + \beta_{cat_3}(x_{cat_3} - \overline{x_{cat_3}} + \overline{x_{cat_3}}) \\
&= (\beta_0 + \beta_{cat_2}\overline{x_{cat_2}} + \beta_{cat_3}\overline{x_{cat_3}}) + \beta_{cat_2}(x_{cat_2} - \overline{x_{cat_2}})  + \beta_{cat_3}(x_{cat_3} - \overline{x_{cat_3}})
\end{split}
$$

If set $\beta_{0,new} = \beta_0 + \beta_{cat_2}\overline{x_{cat_2}} + \beta_{cat_3}\overline{x_{cat_3}}$, then we have: $y = \beta_{0,new} + \beta_{cat_2}(x_{cat_2} - \overline{x_{cat_2}})  + \beta_{cat_3}(x_{cat_3} - \overline{x_{cat_3}})$

Here, $\beta_{0,new}$ should be the mean of $y$ within the entire cohort, $\overline{y}$, let’s check:

$$
\begin{split}
\beta_{0,new} &= \beta_0+\beta_{cat_2}\overline{x_{cat_2}}+\beta_{cat_3}\overline{x_{cat_3}} \\
&= \beta_0+\beta_{cat_2}\frac{n_{cat_2}}{n_{cat_1}+n_{cat_2}+n_{cat_3}}+\beta_{cat_3}\frac{n_{cat_3}}{n_{cat_1}+n_{cat_2}+n_{cat_3}} \\
&= \frac{\beta_0(n_{cat_1}+n_{cat_2}+n_{cat_3}) + \beta_{cat_2}n_{cat_2} + \beta_{cat_3}n_{cat_3}}{n_{cat_1}+n_{cat_2}+n_{cat_3}} \\
&= \frac{\beta_0n_{cat_1} + (\beta_0 + \beta_{cat_2})n_{cat_2} + (\beta_0 + \beta_{cat_3})n_{cat_3}}{n_{cat_1}+n_{cat_2}+n_{cat_3}} \\
&= \overline{y}
\end{split}
$$

Therefore, the centered algorithm can be written as $y = \overline{y} + \beta_{cat_2}(x_{cat_2} - \overline{x_{cat_2}})  + \beta_{cat_3}(x_{cat_3} - \overline{x_{cat_3}})$.

```{r}
#| warning: false
#| message: false

levels(data$type)

# Here I remove `NA`s in `data$type`. Alternatively, those `NA`s can be treated as an extra category.
data <- data %>% filter(!is.na(type)) %>% droplevels()
# create dummy variables
data <- dummy_cols(data, select_columns = "type", remove_first_dummy = TRUE)
prop.table(table(data$type, useNA = "ifany"))

# centering
(x_type_prof_mean <- mean(data$type_prof))
(x_type_wc_mean <- mean(data$type_wc))
data <- data %>% 
  mutate(type_prof_C = type_prof - x_type_prof_mean,
         type_wc_C = type_wc - x_type_wc_mean)
head(data %>% select(contains("type")))

# --- linear regression model on `type` ---"
paste0("Mean prestige score if `type == bc`: ", mean(data$prestige[which(data$type == "bc")]))
paste0("Mean prestige score if `type == prof`: ", mean(data$prestige[which(data$type == "prof")]))
paste0("Mean prestige score if `type == wc`: ", mean(data$prestige[which(data$type == "wc")]))
fit <- lm(prestige ~ type, data = data)
coef(fit)
sum(coef(fit)[c(1,2)]) # beta_0 + beta_cat_2
sum(coef(fit)[c(1,3)]) # beta_0 + beta_cat_3

# --- linear regression model on `type_prof + type_wc` ---"
# confirm that the results are the same as above
fit <- lm(prestige ~ type_prof + type_wc, data = data)
coef(fit)

# --- linear regression model on centered variable `type_prof_C + type_wc_C` ---"
paste0("Mean prestige score in the dataset: ", mean(data$prestige))
fit <- lm(prestige ~ type_prof_C + type_wc_C, data = data)
coef(fit)
```

## Centered algorithm with a *continuous* predictor as it is

Equation of a simple linear regression model: $y = \beta_0 + \beta_1x$. Here, $\beta_0$ is the value of $y$ when $x=0$, and $\beta_1$ is the change of $y$ when $x$ increases by 1. Now, change this into a centered algorithm, we have

$$
\begin{split}
y &= \beta_0 + \beta_1x \\
&= \beta_0 + \beta_1(x-\overline{x}+\overline{x}) \\
&= (\beta_0 + \beta_1\overline{x}) + \beta_1(x-\overline{x})
\end{split}
$$

If set $\beta_{0,new} = \beta_0 + \beta_1\overline{x}$, then we have: $y = \beta_{0,new} + \beta_1(x-\overline{x})$. Note by definition of linear regression, the value of $y$ when $x=\overline{x}$ is also $\beta_0 + \beta_1\overline{x}$.

Here, $\beta_{0,new}$ should be the mean of $y$ within the entire cohort, $\overline{y}$, let’s check:

$$
\begin{split}
\beta_{0,new} &= \beta_0+\beta_1\overline{x} \\
&= \beta_0+\beta_1\frac{\sum_i^n(x_i)}{n} \\
&= \frac{n\beta_0 + \beta_1\sum_i^n(x_i)}{n} \\
&= \frac{\sum_i^n(\beta_0 + \beta_1x_i)}{n} \\
&= \frac{\sum_i^ny_i}{n} \\
&= \overline{y}
\end{split}
$$

**Therefore, the centered algorithm can be written as** $y = \overline{y} + \beta_1(x-\overline{x})$.

```{r}
(x_mean <- mean(data$income))
data <- data %>% 
  mutate(income_C = income - x_mean)

# Using the centered `income_C` (red) instead of `income` (blue) essentially moves the data horizontally to the left by `x_mean`.
ggplot(data, aes(income, prestige)) + 
  geom_point(color = "blue")+
  geom_hline(yintercept = mean(data$prestige), color = "black", linetype = "dashed") +
  annotate("label", x = 21000, y = mean(data$prestige), label = "mean(data$prestige)", size = unit(3, "pt")) + 
  geom_vline(xintercept = mean(data$income), color = "black", linetype = "dashed")+
  annotate("label", x = mean(data$income), y = 100, label = "mean(data$income)", size = unit(3, "pt")) +
  geom_vline(xintercept = 0, linetype = "dashed")+
  geom_smooth(method = "lm", formula = "y ~ x")+
  geom_point(aes(income_C, prestige), color = "red")+
  geom_smooth(aes(income_C, prestige), method = "lm", formula = "y ~ x", color = "red")+
  theme_bw() 

paste0("Mean prestige score in the dataset: ", mean(data$prestige))

# --- linear regression model on `income` ---
fit <- lm(prestige ~ income, data = data)
coef(fit)
coef(fit)[1] + coef(fit)[2] * x_mean

# --- linear regression model on centered variable `income_C` ---
fit <- lm(prestige ~ income_C, data = data)
coef(fit)
```

## Centered algorithm with a *continuous* predictor using *restricted cubic spline (rcs)*

The linear regression with a *rcs* with $k$ knots can be written as $y=\beta_0+\sum_{j=1}^{k-1}{\beta_{rcs_j}x_{rcs_j}}$.

### Components of rcs

Below we use `rms::rcs`, for more information, like the default knot locations, see `?Hmisc::rcspline.eval`.

Equations for restricted cubic spline with $j = 1,...,k$ knots:

$x_{rcs_1} = x,\ j=1$

$x_{rcs_{j}} = {(\frac{x-knot_{j-1}}{(knot_k-knot_1)^{\frac{2}{3}}})}_+^{3} + \frac{(knot_{k-1}-knot_{j-1}){(\frac{x-knot_k}{(knot_k-knot_1)^{\frac{2}{3}}})}_+^{3}- (knot_k-knot_{j-1}){(\frac{x-knot_{k-1}}{(knot_k-knot_1)^{\frac{2}{3}}})}_+^{3}}{knot_k-knot_{k-1}},\ j=2,...,k-1$

where:

$$
(x - knot)_+ = 
\begin{cases}
x - knot&,\ x - knot>0 \\
0&, \ x - knot\le0
\end{cases}
$$

In the example below, we use $k = 4$:

```{r}
#| message: false

k <- 4
# check above formula
rcs.fit <- rcs(data$income, k)
knots <- attributes(rcs.fit)$parms # knot locations
res <- labelled::remove_attributes(rcs.fit, names(attributes(rcs.fit))[3:length(names(attributes(rcs.fit)))]) # don't print the attributes
head(res)

source("R/util.rcs.R", echo = TRUE) 
res <- get_rcs(data$income, knots)
head(res)
```

### Linear regression with *rcs*

However, the coefficients of $y=\beta_0+\sum_{j=1}^{k-1}{\beta_{rcs_j}x_{rcs_j}}$ no longer have the explainability like the simple linear regression. **Note:** based on the knot locations, when $x_{rcs_j}=0$, $\sum_{j=1}^{k-1}{\beta_{rcs_j}x_{rcs_j}}$ is not necessarily equal to $0$, therefore $\beta_0$ doesn't represent the value of $y$ when $x_{rcs_j}=0$. Also, when $x=\overline{x}$, the predicted $y$ is not equal to $\overline{y}$, see the figure below: where the linear regression lines of `prestige ~ income` and `prestige ~ rcs(income,k)` are represented by black and blue, respectively.

```{r}
ggplot(data, aes(income, prestige)) + 
  geom_point(color = "blue")+
  geom_hline(yintercept = mean(data$prestige), color = "red", linetype = "dashed", linewidth = 1) +
  annotate("label", x = 21000, y = mean(data$prestige), label = "mean(data$prestige)", size = unit(3, "pt")) + 
  geom_vline(xintercept = mean(data$income), color = "red", linetype = "dashed", linewidth = 1)+
  annotate("label", x = mean(data$income), y = 100, label = "mean(data$income)", size = unit(3, "pt")) +
  geom_smooth(method = "lm", formula = y ~ x, color = "black", se = F)+
  geom_smooth(method = "lm", formula = y ~ rcs(x,k), color = "blue", se = F)+
  geom_vline(xintercept = attributes(rcs.fit)$parms, color = "blue", linetype = "dashed")+
  annotate("label", x = attributes(rcs.fit)$parms, y = 95, label = paste0("knot ", 1:k), size = unit(3, "pt")) +
  xlim(0, max(data$income)) +
  theme_bw() 
```

### Centered algorithm using *rcs*: using `x_mean`

As expected, the *rcs* of the centered `income_C` is the *rcs* of `income` moving horizontally by `mean(data$income)`, *i.e.*, $\overline{x}$. As for coefficients of regression, the only difference is in `Intercept`, and the difference is by $\beta_{rcs_1}\overline{x}$. Unfortunately, the intercept of the centered algorithm is not equal to $\overline{y}$ like other scenarios above. And the predicted $y$ when $(x-\overline{x}) = 0$ using the centered algorithm is not equal to $\overline{y}$ either.

```{r}
ggplot(data, aes(income, prestige)) + 
  geom_point(color = "blue")+
  geom_hline(yintercept = mean(data$prestige), color = "black", linetype = "dashed") +
  annotate("label", x = 21000, y = mean(data$prestige), label = "mean(data$prestige)", size = unit(3, "pt")) + 
  geom_vline(xintercept = mean(data$income), color = "black", linetype = "dashed")+
  annotate("label", x = mean(data$income), y = 100, label = "mean(data$income)", size = unit(3, "pt")) +
  geom_vline(xintercept = 0, linetype = "dashed")+
  geom_smooth(method = "lm", formula = y ~ rcs(x,k))+
  geom_point(aes(income_C, prestige), color = "red")+
  geom_smooth(aes(income_C, prestige), method = "lm", formula = y ~ rcs(x,k), color = "red")+
  theme_bw() 

rcs.fit <- rcs(data$income, k)
# knot locations
attributes(rcs.fit)$parms

# knot locations when using `income_C`
rcs.fit.C <- rcs(data$income_C, k)
attributes(rcs.fit.C)$parms
attributes(rcs.fit.C)$parms + x_mean

# And only the `data` column is affected, 
# for other columns, the horizontal change was cancelled out.
head(labelled::remove_attributes(rcs.fit, names(attributes(rcs.fit))[3:length(names(attributes(rcs.fit)))])) # don't print the attributes
head(labelled::remove_attributes(rcs.fit.C, names(attributes(rcs.fit.C))[3:length(names(attributes(rcs.fit.C)))])) # don't print the attributes
```

```{r}
# --- linear regression model on `rcs(income,k)` ---
fit <- lm(prestige ~ rcs(income,k), data = data)
coef(fit)

# --- linear regression model on centered variable `rcs(income_C,k)` ---
fit.C <- lm(prestige ~ rcs(income_C,k), data = data)
coef(fit.C)

# difference between the two intercepts
coef(fit)[1] + coef(fit)[2]*x_mean

# prediction using the centered algorithm when x = 0
paste0("Mean prestige score in the dataset: ", mean(data$prestige))
predict(fit.C, newdata = data.frame(income_C=0))
```

### Centered algorithm using *rcs*: using an offset instead of `x_mean`

From above, it confirms that when doing a horizontal shift, all $\beta$s except for the intercept remain unaffected. Therefore, the current task becomes to find a value $\tilde{x}$ that $\tilde{y}=\beta_0+\sum_{j=1}^{k-1}{\beta_{rcs_j}\tilde{x}_{rcs_j}}= \overline{y}$ (*i.e.* the "root" below), and use it as the offset.

```{r}
root.income <- uniroot.all(fun_rcs, c(min(data$income),max(data$income)), object = fit, y_mean = mean(data$prestige), varname = "income")

# the "centered" income
data <- data %>% mutate(income_C2 = income-root.income)

ggplot(data, aes(income, prestige)) + 
  geom_point(color = "blue")+
  geom_hline(yintercept = mean(data$prestige), color = "black", linetype = "dashed") +
  annotate("label", x = 21000, y = mean(data$prestige), label = "mean(data$prestige)", size = unit(3, "pt")) + 
  geom_vline(xintercept = 0, linetype = "dashed")+
  geom_smooth(method = "lm", formula = y ~ rcs(x,k), se = F)+
  geom_vline(xintercept = mean(data$income), color = "red", linetype = "dashed")+
  annotate("label", x = mean(data$income), y = 100, label = "mean(data$income)", size = unit(3, "pt")) +
  geom_point(aes(income_C, prestige), color = "red")+
  geom_smooth(aes(income_C, prestige), method = "lm", formula = y ~ rcs(x,k), color = "red", se = F)+
  geom_vline(xintercept = root.income, color = "forestgreen", linetype = "dashed")+
  annotate("label", x = root.income, y = 95, label = "root.income", size = unit(3, "pt")) +
  geom_point(aes(income_C2, prestige), color = "forestgreen")+
  geom_smooth(aes(income_C2, prestige), method = "lm", formula = y ~ rcs(x,k), color = "forestgreen", se = F)+
  theme_bw() 
```

Note here, the intercept is not exactly equal to $\overline{y}$, but the fitted value when `x == root.income`, *i.e.* `x-root.income == 0` is equal to $\overline{y}$.

```{r}
fit.C2 <- lm(prestige ~ rcs(income_C2,k), data = data)
coef(fit.C2)

# prediction using the centered algorithm when x = 0
paste0("Mean prestige score in the dataset: ", mean(data$prestige))
predict(fit.C2, newdata = data.frame(income_C2=0))
```

# Two predictors, no interaction

## Centered algorithm for two *categorical* predictors

I will not repeat the math here, just use below code to confirm that the intercept ($\beta_0$) of the centered algorithm is still $\overline{y}$, and values of other coefficients are not affected by centering.

```{r}
# Note here we use the dataset after removing `NA`s in data$type
# create a new categrical variable
data <- data %>% 
  mutate(income_cat = case_when(
    income <= 5000 ~ "low",
    income <= 10000 ~ "medium",
    TRUE ~ "high"
  ))
data$income_cat <- factor(data$income_cat, levels = c("low", "medium", "high"))
# create dummy variables
data <- dummy_cols(data, select_columns = "income_cat", remove_first_dummy = TRUE)

# centering
x_income_cat_medium_mean <- mean(data$income_cat_medium)
x_income_cat_high_mean <- mean(data$income_cat_high)
data <- data %>% 
  mutate(income_cat_medium_C = income_cat_medium - x_income_cat_medium_mean,
         income_cat_high_C = income_cat_high - x_income_cat_high_mean)

# --- linear regression model on `type` and `income_cat` ---"
fit <- lm(prestige ~ type + income_cat, data = data)
coef(fit)

# --- linear regression model on centered variable ---"
paste0("Mean prestige score in the dataset: ", mean(data$prestige))
fit <- lm(prestige ~ type_prof_C + type_wc_C + income_cat_medium_C + income_cat_high_C, data = data)
coef(fit)
```

## Centered algorithm for two *continuous* predictors

I will not repeat the math here, just use below code to confirm that the intercept ($\beta_0$) of the centered algorithm is still $\overline{y}$, and values of other coefficients are not affected by centering.

```{r}
x_mean <- mean(data$education)
data <- data %>% 
  mutate(education_C = education - x_mean)

# --- linear regression model on `income` and `education` ---
fit <- lm(prestige ~ income + education, data = data)
coef(fit)

# --- linear regression model on centered variable `income_C` and `education_C` ---
paste0("Mean prestige score in the dataset: ", mean(data$prestige))
fit <- lm(prestige ~ income_C + education_C, data = data)
coef(fit)
```

## Centered algorithm for a *categorical* predictor and a *continuous* predictor

I will not repeat the math here, just use below code to confirm that the intercept ($\beta_0$) of the centered algorithm is still $\overline{y}$, and values of other coefficients are not affected by centering.

```{r}
# --- linear regression model on `income` ---
fit <- lm(prestige ~ income + type, data = data)
coef(fit)

# --- linear regression model on centered variable `income_C` ---
paste0("Mean prestige score in the dataset: ", mean(data$prestige))
fit <- lm(prestige ~ income_C + type_prof_C + type_wc_C, data = data)
coef(fit)
```

## Centered algorithm for two *continuous* predictors with *rcs*

I will not repeat the math here, just use below code to confirm that values of coefficients other than the intercepts are not affected by centering.

```{r}
fit <- lm(prestige ~ rcs(education,k), data = data)
root.education <- uniroot.all(fun_rcs, c(min(data$education),max(data$education)), object = fit, y_mean = mean(data$prestige), varname = "education")
data <- data %>% 
  mutate(education_C2 = education - root.education)

# --- linear regression model on `income` and `education` ---
fit <- lm(prestige ~ rcs(income,k) + rcs(education,k), data = data)
coef(fit)

# --- linear regression model on centered variable `income_C` and `education_C` ---
fit <- lm(prestige ~ rcs(income_C2,k) + rcs(education_C2,k), data = data)
coef(fit)
predict(fit, newdata = data.frame(income_C2 = 0, education_C2 = 0))
```

## Centered algorithm for a *categorical* predictor and a *continuous* predictor with *rcs*

I will not repeat the math here, just use below code to confirm that values of coefficients other than the intercepts are not affected by centering.

```{r}
# --- linear regression model on `income` ---
fit <- lm(prestige ~ rcs(income,k) + type, data = data)
coef(fit)

# --- linear regression model on centered variable `income_C` ---
fit_C <- lm(prestige ~ rcs(income_C2,k) + type_prof_C + type_wc_C, data = data)
coef(fit_C)
predict(fit_C, newdata = data.frame(income_C2 = 0, type_prof_C = 0, type_wc_C = 0))
```

# Two predictors, with interaction

**Note:** `lm(outcome ~ predictor1 + predictor2 + predictor1*predictor2, data)` and `lm(outcome ~ predictor1*predictor2, data)` are the same.

**Question:** in MPort `~rcs(age_C,5) + smk_cat2_C + smk_cat3_C + smk_cat4_C +...+ age*smk_cat2_C + age*smk_cat3_C + age*smk_cat4_C +...`?

```{r}
#| fig-height: 4
#| fig-width: 8
#| message: false

# --- Original variables --- 
# no interaction
fit_wo   <- lm(prestige ~ type + income, data = data)
data$pred_wo <- predict(fit_wo,data)
# with interaction
fit_wt   <- lm(prestige ~ type*income, data = data)
data$pred_wt <- predict(fit_wt,data)

p1 <- ggplot(data, aes(income, prestige, color = type)) +
  geom_point(alpha = 0.5)+
  geom_smooth(method = "lm", mapping=aes(y=pred_wo), se = F) +
  geom_smooth(method = "lm", mapping=aes(y=pred_wo), group = 1, color = "black", se = F) +
  annotate("label", x = 23000, y = 100, label = "prestige ~ income", size = unit(3, "pt")) + 
  geom_hline(yintercept = mean(data$prestige), color = "black", linetype = "dashed") +
  annotate("label", x = 21000, y = mean(data$prestige), label = "mean(data$prestige)", size = unit(3, "pt")) + 
  geom_vline(xintercept = mean(data$income), color = "black", linetype = "dashed")+
  annotate("label", x = mean(data$income), y = 100, label = "mean(data$income)", size = unit(3, "pt")) +
  theme_bw()+
  labs(title = "type + income")
p2 <- ggplot(data, aes(income, prestige, color = type)) +
  geom_point(alpha = 0.5)+
  geom_smooth(method = "lm", mapping=aes(y=pred_wt), se = F) +
  geom_smooth(method = "lm", mapping=aes(y=pred_wo), group = 1, color = "black", se = F) +
  annotate("label", x = 23000, y = 100, label = "prestige ~ income", size = unit(3, "pt")) + 
  geom_hline(yintercept = mean(data$prestige), color = "black", linetype = "dashed") +
  annotate("label", x = 21000, y = mean(data$prestige), label = "mean(data$prestige)", size = unit(3, "pt")) + 
  geom_vline(xintercept = mean(data$income), color = "black", linetype = "dashed")+
  annotate("label", x = mean(data$income), y = 100, label = "mean(data$income)", size = unit(3, "pt")) +
  theme_bw()+
  labs(title = "type*income")
ggarrange(p1, p2, ncol = 2, common.legend = T)
```

## Linear regression with interaction

Essentially, the linear regression with interaction is equivalent to fit separately within each group. Suppose $x_{cat}$ is a categorical variable with $3$ levels $cat_1$, $cat_2$, and $cat_3$, and $x_{con}$ is a continuous variable. When fit $3$ linear regression, we have:

$y = \beta_{0_{cat_1}} + \beta_{1_{cat_1}}x_{con},\ x_{cat} = cat_1$ (Eq.1)

$y = \beta_{0_{cat_2}} + \beta_{1_{cat_2}}x_{con},\ x_{cat} = cat_2$ (Eq.2)

$y = \beta_{0_{cat_3}} + \beta_{1_{cat_3}}x_{con},\ x_{cat} = cat_3$ (Eq.3)

The the linear regression with interaction can be written as (with $x_{cat}$ transformed into 2 dummy variables $x_{cat_2}$ and $x_{cat_3}$):

$$
\begin{split}
y = &\beta_{0_{cat_1}} + (\beta_{0_{cat_2}} - \beta_{0_{cat_1}})x_{cat_2} + (\beta_{0_{cat_3}} - \beta_{0_{cat_1}})x_{cat_3} + \\
&\beta_{1_{cat_1}}x_{con} + (\beta_{1_{cat_2}} - \beta_{1_{cat_1}})x_{con}*x_{cat_2} + (\beta_{1_{cat_3}} - \beta_{1_{cat_1}})x_{con}*x_{cat_3}
\end{split}
$$

Therefore, when $x_{cat} = cat_1$, $x_{cat_2}=x_{cat_3}=0$, the above equation becomes Eq.1; when $x_{cat_2}=1$ and $x_{cat_3}=0$, Eq.2; when $x_{cat_2}=0$ and $x_{cat_3}=1$, Eq.3.

```{r}
fit_interaction   <- lm(prestige ~ type*income, data = data)
coef(fit_interaction)

fit_bc <- lm(prestige ~ income, data = data %>% filter(type =="bc"))
coef(fit_bc)

fit_prof <- lm(prestige ~ income, data = data %>% filter(type =="prof"))
coef(fit_prof)
coef(fit_interaction)[1] + coef(fit_interaction)[2]
coef(fit_interaction)[4] + coef(fit_interaction)[5]

fit_wc <- lm(prestige ~ income, data = data %>% filter(type =="wc"))
coef(fit_wc)
coef(fit_interaction)[1] + coef(fit_interaction)[3]
coef(fit_interaction)[4] + coef(fit_interaction)[6]
```

## Centered linear regression with interaction

If we use separate $\overline{x}$ in each group (Eq.1 to Eq.3), then we can get each corresponding $\overline{y}$. However, when we use $\overline{x}$ of the entire cohort, we can't get the overall $\overline{y}$ or the subgroup $\overline{y}$s. Let alone the added complexity when we use the centered categorical (dummy) variable as well.

```{r}
#| message: false

ggplot(data, aes(income, prestige, color = type)) +
  geom_point(alpha = 0.5)+
  geom_smooth(method = "lm", mapping=aes(y=pred_wt), se = F) +
  geom_hline(yintercept = mean(data$prestige[which(data$type == "bc")]), color = "#F8766D", linetype = "dashed") +
  annotate("label", x = 21000, y = mean(data$prestige[which(data$type == "bc")]), label = "mean(data$prestige), type = bc", size = unit(3, "pt"), color = "#F8766D") + 
  geom_vline(xintercept = mean(data$income[which(data$type == "bc")]), color = "#F8766D", linetype = "dashed")+
  annotate("label", x = mean(data$income[which(data$type == "bc")]), y = 105, label = "mean(data$income), type = bc", size = unit(3, "pt"), color = "#F8766D") +
  
  geom_hline(yintercept = mean(data$prestige[which(data$type == "prof")]), color = "#00BA38", linetype = "dashed") +
  annotate("label", x = 21000, y = mean(data$prestige[which(data$type == "prof")]), label = "mean(data$prestige), type = prof", size = unit(3, "pt"), color = "#00BA38") + 
  geom_vline(xintercept = mean(data$income[which(data$type == "prof")]), color = "#00BA38", linetype = "dashed")+
  annotate("label", x = mean(data$income[which(data$type == "prof")]), y = 95, label = "mean(data$income), type = prof", size = unit(3, "pt"), color = "#00BA38") +
  
  geom_hline(yintercept = mean(data$prestige[which(data$type == "wc")]), color = "#619CFF", linetype = "dashed") +
  annotate("label", x = 21000, y = mean(data$prestige[which(data$type == "wc")]), label = "mean(data$prestige), type = wc", size = unit(3, "pt"), color = "#619CFF") + 
  geom_vline(xintercept = mean(data$income[which(data$type == "wc")]), color = "#619CFF", linetype = "dashed")+
  annotate("label", x = mean(data$income[which(data$type == "wc")]), y = 110, label = "mean(data$income), type = wc", size = unit(3, "pt"), color = "#619CFF") +
  
  geom_smooth(method = "lm", mapping=aes(y=pred_wo), group = 1, color = "black", se = F) +
  annotate("label", x = 24500, y = 100, label = "prestige ~ income", size = unit(3, "pt")) + 
  geom_hline(yintercept = mean(data$prestige), color = "black", linetype = "dashed") +
  annotate("label", x = 21000, y = mean(data$prestige), label = "mean(data$prestige)", size = unit(3, "pt")) + 
  geom_vline(xintercept = mean(data$income), color = "black", linetype = "dashed")+
  annotate("label", x = mean(data$income), y = 100, label = "mean(data$income)", size = unit(3, "pt")) +
  theme_bw()+
  labs(title = "type*income")
```

```{r}
fit_interaction   <- lm(prestige ~ income*type_prof + income*type_wc, data = data) # the same as `prestige ~ type*income`
coef(fit_interaction)

fit_C_interaction   <- lm(prestige ~ income_C*type_prof_C + income_C*type_wc_C, data = data)
coef(fit_C_interaction)

paste0("Mean prestige score in the dataset: ", mean(data$prestige))
paste0("Mean prestige score when type == bc: ", mean(data$prestige[which(data$type == "bc")]))
paste0("Mean prestige score when type == prof: ", mean(data$prestige[which(data$type == "prof")]))
paste0("Mean prestige score when type == wc: ", mean(data$prestige[which(data$type == "wc")]))
```

```{r}

```