---
title: "Centered algorithm with simulated data, without interaction"
author: "Juan Li"
date: 2025-08-05
format: 
  html:
    toc: true
    html-math-method: katex
    css: styles.css
editor: visual
---

```{r}
#| warning: false
#| message: false
#| output: false

library(dplyr) # Data manipulation
library(ggplot2) # Data visualization
library(ggpubr) # ggarrange
library(fastDummies) # make dummy variables
library(rms)

source("R/util.rcs.R")
```

# Simulate data for this tutorial

Resources:

-   [Creating simulated data sets in R](https://stirlingcodingclub.github.io/simulating_data/index.html)

-   [Generate Simulated Dataset for Linear Model in R](https://towardsdatascience.com/generate-simulated-dataset-for-linear-model-in-r-469a5e2f4c2e/)

For this tutorial, we will use a simulated dataset with the following predictors:

-   `x1`: a categorical/dichotomous variable with 2 categories: `x1_cat1`, `x1_cat2`

-   `x2`: a categorical variable with 3 categories: `x2_cat1`, `x2_cat2`, `x2_cat3`

-   `x3`: a continuous variable with normal distribution, used as it is

-   `x4`: a continuous variablewith normal distribution, used with restricted cubic spline (rcs), `k_x4 = 4`

-   `x5`: a continuous variable with normal distribution, used as it is

-   `x6`: a continuous variablewith normal distribution, used with restricted cubic spline (rcs), `k_x6 = 3`

And we assume the outcome `y` is a linear combination of the predictors with some noise:

$$
\begin{split}
y = & \beta_0 + \\
& \beta_{1_{cat2}}x_{1_{cat2}} + \\
& \beta_{2_{cat2}}x_{2_{cat2}} + \beta_{2_{cat3}}x_{2_{cat3}} + \\
& \beta_3x_3 + \\
& \sum_{j=1}^{k_{x4}-1}{\beta_{4_{rcs_j}}x_{4_{rcs_j}}} + \\
& \beta_5x_5 + \\
& \sum_{j=1}^{k_{x6}-1}{\beta_{6_{rcs_j}}x_{6_{rcs_j}}} 
\end{split}
$$

```{r}
n <- 1000
seed <- 100

set.seed(seed)
data <- data.frame(
  x1 = sample(
    c("x1_cat1", "x1_cat2"),
    size = n,
    replace = TRUE,
    prob = c(0.5, 0.5)
  ),
  x2 = sample(
    c("x2_cat1", "x2_cat2", "x2_cat3"),
    size = n,
    replace = TRUE,
    prob = c(0.1, 0.4, 0.5)
  ),
  x3 = rnorm(n, mean = 100, sd = 38),
  x4 = rnorm(n, mean = 50, sd = 15),
  x5 = rnorm(n, mean = 30, sd = 5),
  x6 = rnorm(n, mean = 15, sd = 3),
  error = rnorm(n, mean = 0, sd = 3)
)
table(data$x1)
table(data$x2)

# dummy variables
data <- data %>%
  mutate(
    x1_cat2 = ifelse(x1 == "x1_cat2", 1, 0),
    x2_cat2 = ifelse(x2 == "x2_cat2", 1, 0),
    x2_cat3 = ifelse(x2 == "x2_cat3", 1, 0)
  )

# rcs
k_x4 <- 4
rcs.fit <- rcs(data$x4, k_x4)
rcs.fit_x4 <- labelled::remove_attributes(
  rcs.fit,
  names(attributes(rcs.fit))[3:length(names(attributes(rcs.fit)))]
)
colnames(rcs.fit_x4) <- paste0("x4_rcs_", 1:(k_x4 - 1))

k_x6 <- 3
rcs.fit <- rcs(data$x6, k_x6)
rcs.fit_x6 <- labelled::remove_attributes(
  rcs.fit,
  names(attributes(rcs.fit))[3:length(names(attributes(rcs.fit)))]
)
colnames(rcs.fit_x6) <- paste0("x6_rcs_", 1:(k_x6 - 1))

data <- bind_cols(data, rcs.fit_x4, rcs.fit_x6)

# outcome
data <- data %>%
  mutate(
    y = 100 +
      3 * x1_cat2 +
      2 * x2_cat2 +
      0.5 * x2_cat3 +
      10 * x3 +
      25 * x4_rcs_1 +
      18 * x4_rcs_2 +
      21 * x4_rcs_3 -
      6 * x5 +
      0.6 * x6_rcs_1 +
      1.5 * x6_rcs_2 +
      error
  )

head(data)

# check
fit <- lm(y ~ x1 + x2 + x3 + rcs(x4, k_x4) + x5 + rcs(x6, k_x6), data = data)
coef(fit)
```

# One predictor

## Centered algorithm with a *dichotomous* predictor

Equation of a simple linear regression model: $y = \beta_0 + \beta_1x,\ x=0\ or\ 1$. Here, $\beta_0$ is the mean of $y$ when $x=0$, and $\beta_0 + \beta_1$ is the mean of $y$ when $x=1$. Now, change this into a centered algorithm, and the mean of $x$ is $\overline{x} = \frac{n_{x=1}}{n_{x=0}+n_{x=1}}$, we have

$$
\begin{split}
y &= \beta_0 + \beta_1x \\
&= \beta_0 + \beta_1(x-\overline{x}+\overline{x}) \\
&= (\beta_0 + \beta_1\overline{x}) + \beta_1(x-\overline{x})
\end{split}
$$

If set $\beta_{0,new} = \beta_0 + \beta_1\overline{x}$, then we have: $y = \beta_{0,new} + \beta_1(x-\overline{x})$

Here, $\beta_{0,new}$ should be the mean of $y$ within the entire cohort, $\overline{y}$, let’s check:

$$
\begin{split}
\beta_{0,new} 
&= \beta_0+\beta_1\overline{x} \\
&= \beta_0+\beta_1\frac{n_{x=1}}{n_{x=0}+n_{x=1}} \\
&= \frac{\beta_0(n_{x=0}+n_{x=1}) + \beta_1n_{x=1}}{n_{x=0}+n_{x=1}} \\
&= \frac{\beta_0n_{x=0} + (\beta_0 + \beta_1)n_{x=1}}{n_{x=0}+n_{x=1}} \\&= \overline{y}
\end{split}
$$

**Therefore, the centered algorithm can be written as** $y = \overline{y} + \beta_1(x-\overline{x})$.

```{r}
#| warning: false
#| message: false

# centering
prop.table(table(data$x1))
(x_mean <- mean(data$x1_cat2))
data <- data %>%
  mutate(x1_cat2_C = x1_cat2 - x_mean)

# --- linear regression model on `x1_cat2` ---
paste0(
  "Mean outcome if `x1_cat2 == 0`: ",
  mean(data$y[which(data$x1_cat2 == 0)])
)
paste0(
  "Mean outcome if `x1_cat2 == 1`: ",
  mean(data$y[which(data$x1_cat2 == 1)])
)
fit <- lm(y ~ x1, data = data)
coef(fit)
sum(coef(fit)) # beta_0 + beta_1

# --- linear regression model on centered variable `x1_C` ---"
paste0("Mean outcome in the dataset: ", mean(data$y))
fit <- lm(y ~ x1_cat2_C, data = data)
coef(fit)
```

## Centered algorithm with a *categorical* predictor

First, the categorical variable needs to be transformed into dummy variables. A categorical variable $x$ with $m$ levels: $cat_1$, $cat_2$,..., $cat_m$, can be transformed into $m-1$ dummy variables. Suppose $m=3$, then we have:

$$
x_{cat_2} =
\begin{cases}
1 ,& x = cat_2 \\
0  ,& otherwise
\end{cases}       
$$

and

$$
x_{cat_3} =
\begin{cases}
1 ,& x = cat_3 \\
0  ,& otherwise
\end{cases}       
$$

Then, when $x = cat_1$, $x_{cat_2} = x_{cat_3} = 0$.

Equation of a simple linear regression model: $y = \beta_0 + \beta_{cat_2}x_{cat_2}+ \beta_{cat_3}x_{cat_3}$. Here, $\beta_0$ is the mean of $y$ when $x = cat_1$, $\beta_0 + \beta_{cat_2}$ is the mean of $y$ when $x=cat_2$, and $\beta_0 + \beta_{cat_3}$ is the mean of $y$ when $x=cat_3$. Now, change this into a centered algorithm:

$$
\begin{split}
y &= \beta_0 + \beta_{cat_2}x_{cat_2} + \beta_{cat_3}x_{cat_3} \\
&= \beta_0 + \beta_{cat_2}(x_{cat_2} - \overline{x_{cat_2}} + \overline{x_{cat_2}})  + \beta_{cat_3}(x_{cat_3} - \overline{x_{cat_3}} + \overline{x_{cat_3}}) \\
&= (\beta_0 + \beta_{cat_2}\overline{x_{cat_2}} + \beta_{cat_3}\overline{x_{cat_3}}) + \beta_{cat_2}(x_{cat_2} - \overline{x_{cat_2}})  + \beta_{cat_3}(x_{cat_3} - \overline{x_{cat_3}})
\end{split}
$$

If set $\beta_{0,new} = \beta_0 + \beta_{cat_2}\overline{x_{cat_2}} + \beta_{cat_3}\overline{x_{cat_3}}$, then we have: $y = \beta_{0,new} + \beta_{cat_2}(x_{cat_2} - \overline{x_{cat_2}})  + \beta_{cat_3}(x_{cat_3} - \overline{x_{cat_3}})$

Here, $\beta_{0,new}$ should be the mean of $y$ within the entire cohort, $\overline{y}$, let’s check:

$$
\begin{split}
\beta_{0,new} &= \beta_0+\beta_{cat_2}\overline{x_{cat_2}}+\beta_{cat_3}\overline{x_{cat_3}} \\
&= \beta_0+\beta_{cat_2}\frac{n_{cat_2}}{n_{cat_1}+n_{cat_2}+n_{cat_3}}+\beta_{cat_3}\frac{n_{cat_3}}{n_{cat_1}+n_{cat_2}+n_{cat_3}} \\
&= \frac{\beta_0(n_{cat_1}+n_{cat_2}+n_{cat_3}) + \beta_{cat_2}n_{cat_2} + \beta_{cat_3}n_{cat_3}}{n_{cat_1}+n_{cat_2}+n_{cat_3}} \\
&= \frac{\beta_0n_{cat_1} + (\beta_0 + \beta_{cat_2})n_{cat_2} + (\beta_0 + \beta_{cat_3})n_{cat_3}}{n_{cat_1}+n_{cat_2}+n_{cat_3}} \\
&= \overline{y}
\end{split}
$$

Therefore, the centered algorithm can be written as $y = \overline{y} + \beta_{cat_2}(x_{cat_2} - \overline{x_{cat_2}})  + \beta_{cat_3}(x_{cat_3} - \overline{x_{cat_3}})$.

```{r}
#| warning: false
#| message: false

prop.table(table(data$x2))

# centering
(x2_cat2_mean <- mean(data$x2_cat2))
(x2_cat3_mean <- mean(data$x2_cat3))
data <- data %>%
  mutate(x2_cat2_C = x2_cat2 - x2_cat2_mean, x2_cat3_C = x2_cat3 - x2_cat3_mean)

# --- linear regression model on `x2` ---"
paste0(
  "Mean outcome if `x2 == x2_cat1`: ",
  mean(data$y[which(data$x2 == "x2_cat1")])
)
paste0(
  "Mean outcome if `x2 == x2_cat2`: ",
  mean(data$y[which(data$x2 == "x2_cat2")])
)
paste0(
  "Mean outcome if `x2 == x2_cat3`: ",
  mean(data$y[which(data$x2 == "x2_cat3")])
)
fit <- lm(y ~ x2, data = data)
coef(fit)
sum(coef(fit)[c(1, 2)]) # beta_0 + beta_cat_2
sum(coef(fit)[c(1, 3)]) # beta_0 + beta_cat_3

# --- linear regression model on `x2_cat2 + x2_cat3` ---"
# confirm that the results are the same as above
fit <- lm(y ~ x2_cat2 + x2_cat3, data = data)
coef(fit)

# --- linear regression model on centered variable `x2_cat2_C + x2_cat3_C` ---"
paste0("Mean outcome in the dataset: ", mean(data$y))
fit <- lm(y ~ x2_cat2_C + x2_cat3_C, data = data)
coef(fit)
```

## Centered algorithm with a *continuous* predictor as it is

Equation of a simple linear regression model: $y = \beta_0 + \beta_1x$. Here, $\beta_0$ is the value of $y$ when $x=0$, and $\beta_1$ is the change of $y$ when $x$ increases by 1. Now, change this into a centered algorithm, we have

$$
\begin{split}
y &= \beta_0 + \beta_1x \\
&= \beta_0 + \beta_1(x-\overline{x}+\overline{x}) \\
&= (\beta_0 + \beta_1\overline{x}) + \beta_1(x-\overline{x})
\end{split}
$$

If set $\beta_{0,new} = \beta_0 + \beta_1\overline{x}$, then we have: $y = \beta_{0,new} + \beta_1(x-\overline{x})$. Note by definition of linear regression, the value of $y$ when $x=\overline{x}$ is also $\beta_0 + \beta_1\overline{x}$.

Here, $\beta_{0,new}$ should be the mean of $y$ within the entire cohort, $\overline{y}$, let’s check:

$$
\begin{split}
\beta_{0,new} &= \beta_0+\beta_1\overline{x} \\
&= \beta_0+\beta_1\frac{\sum_i^n(x_i)}{n} \\
&= \frac{n\beta_0 + \beta_1\sum_i^n(x_i)}{n} \\
&= \frac{\sum_i^n(\beta_0 + \beta_1x_i)}{n} \\
&= \frac{\sum_i^ny_i}{n} \\
&= \overline{y}
\end{split}
$$

**Therefore, the centered algorithm can be written as** $y = \overline{y} + \beta_1(x-\overline{x})$.

```{r}
(x_mean <- mean(data$x3))
data <- data %>%
  mutate(x3_C = x3 - x_mean)

# Using the centered `x3_C` (red) instead of `x3` (blue) essentially moves the data horizontally to the left by `x_mean`.
ggplot(data, aes(x3, y)) +
  geom_point(color = "blue", alpha = 0.1) +
  geom_hline(yintercept = mean(data$y), color = "black", linetype = "dashed") +
  annotate(
    "label",
    x = max(data$x3),
    y = mean(data$y),
    label = "mean(data$y)",
    size = unit(3, "pt")
  ) +
  geom_vline(xintercept = mean(data$x3), color = "black", linetype = "dashed") +
  annotate(
    "label",
    x = mean(data$x3),
    y = max(data$y),
    label = "mean(data$x3)",
    size = unit(3, "pt")
  ) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_smooth(method = "lm", formula = "y ~ x") +
  geom_point(aes(x3_C, y), color = "red", alpha = 0.1) +
  geom_smooth(aes(x3_C, y), method = "lm", formula = "y ~ x", color = "red") +
  theme_bw()

paste0("Mean outcome in the dataset: ", mean(data$y))

# --- linear regression model on `x3` ---
fit <- lm(y ~ x3, data = data)
coef(fit)
coef(fit)[1] + coef(fit)[2] * x_mean

# --- linear regression model on centered variable `x3_C` ---
fit <- lm(y ~ x3_C, data = data)
coef(fit)
```

## Centered algorithm with a *continuous* predictor using *restricted cubic spline (rcs)*

**Note:** this is a newer version where treat rcs as (k-1) independent variables. For the original idea that to find the root as the offest instead of `x_mean`, please see the .qmd file with the same file name in the `backup` folder.

The centered algorithm with a *rcs* is similar to

The linear regression with a *rcs* with $k$ knots can be written as $y=\beta_0+\sum_{j=1}^{k-1}{\beta_{rcs_j}x_{rcs_j}}$.

### Components of rcs

Below we use `rms::rcs`, for more information, like the default knot locations, see `?Hmisc::rcspline.eval`.

Equations for restricted cubic spline with $j = 1,...,k$ knots:

$x_{rcs_1} = x,\ j=1$

$x_{rcs_{j}} = {(\frac{x-knot_{j-1}}{(knot_k-knot_1)^{\frac{2}{3}}})}_+^{3} + \frac{(knot_{k-1}-knot_{j-1}){(\frac{x-knot_k}{(knot_k-knot_1)^{\frac{2}{3}}})}_+^{3}- (knot_k-knot_{j-1}){(\frac{x-knot_{k-1}}{(knot_k-knot_1)^{\frac{2}{3}}})}_+^{3}}{knot_k-knot_{k-1}},\ j=2,...,k-1$

where:

$$
(x - knot)_+ = 
\begin{cases}
x - knot&,\ x - knot>0 \\
0&, \ x - knot\le0
\end{cases}
$$

In the example below, we use $k = 4$:

```{r}
#| message: false

# check above formula
rcs.fit <- rcs(data$x4, k_x4)
knots <- attributes(rcs.fit)$parms # knot locations
res <- labelled::remove_attributes(
  rcs.fit,
  names(attributes(rcs.fit))[3:length(names(attributes(rcs.fit)))]
) # don't print the attributes
print(head(res))

source("R/util.rcs.R", echo = TRUE)
res <- get_rcs(data$x4, knots)
print(head(res))
```

### Linear regression with *rcs*

However, the coefficients of $y=\beta_0+\sum_{j=1}^{k-1}{\beta_{rcs_j}x_{rcs_j}}$ no longer have the explainability like the simple linear regression. **Note:** based on the knot locations, when $x_{rcs_j}=0$, $\sum_{j=1}^{k-1}{\beta_{rcs_j}x_{rcs_j}}$ is not necessarily equal to $0$, therefore $\beta_0$ doesn't represent the value of $y$ when $x_{rcs_j}=0$. Also, when $x=\overline{x}$, the predicted $y$ is not equal to $\overline{y}$, see the figure below: where the linear regression lines of `y ~ x4` and `y ~ rcs(x4,k)` are represented by black and blue, respectively.

```{r}
ggplot(data, aes(x4, y)) +
  geom_point(color = "blue", alpha = 0.1) +
  geom_hline(
    yintercept = mean(data$y),
    color = "red",
    linetype = "dashed",
    linewidth = 1
  ) +
  annotate(
    "label",
    x = max(data$x4),
    y = mean(data$y),
    label = "mean(data$y)",
    size = unit(3, "pt")
  ) +
  geom_vline(
    xintercept = mean(data$x4),
    color = "red",
    linetype = "dashed",
    linewidth = 1
  ) +
  annotate(
    "label",
    x = mean(data$x4),
    y = max(data$y),
    label = "mean(data$x4)",
    size = unit(3, "pt")
  ) +
  geom_smooth(method = "lm", formula = y ~ x, color = "black", se = F) +
  geom_smooth(
    method = "lm",
    formula = y ~ rcs(x, k_x4),
    color = "blue",
    se = F
  ) +
  geom_vline(
    xintercept = attributes(rcs.fit)$parms,
    color = "blue",
    linetype = "dashed"
  ) +
  annotate(
    "label",
    x = attributes(rcs.fit)$parms,
    y = max(data$y) * 0.9,
    label = paste0("knot ", 1:k_x4),
    size = unit(3, "pt")
  ) +
  theme_bw()
```

### Centered algorithm using *rcs*: using `x_mean`

As expected, the *rcs* of the centered `x4_C` is the *rcs* of `x4` moving horizontally by `mean(data$x4)`, *i.e.*, $\overline{x}$. As for coefficients of regression, the only difference is in `Intercept`, and the difference is by $\beta_{rcs_1}\overline{x}$. Unfortunately, the intercept of the centered algorithm is not equal to $\overline{y}$ like other scenarios above. And the predicted $y$ when $(x-\overline{x}) = 0$ using the centered algorithm is not equal to $\overline{y}$ either.

```{r}
(x_mean <- mean(data$x4))
data <- data %>%
  mutate(x4_C = x4 - x_mean)

ggplot(data, aes(x4, y)) +
  geom_point(color = "blue", alpha = 0.1) +
  geom_hline(yintercept = mean(data$y), color = "black", linetype = "dashed") +
  annotate(
    "label",
    x = max(data$x4),
    y = mean(data$y),
    label = "mean(data$y)",
    size = unit(3, "pt")
  ) +
  geom_vline(xintercept = mean(data$x4), color = "black", linetype = "dashed") +
  annotate(
    "label",
    x = mean(data$x4),
    y = max(data$y),
    label = "mean(data$x4)",
    size = unit(3, "pt")
  ) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_smooth(method = "lm", formula = y ~ rcs(x, k_x4)) +
  geom_point(aes(x4_C, y), color = "red", alpha = 0.1) +
  geom_smooth(
    aes(x4_C, y),
    method = "lm",
    formula = y ~ rcs(x, k_x4),
    color = "red"
  ) +
  theme_bw()

rcs.fit <- rcs(data$x4, k_x4)
# knot locations
attributes(rcs.fit)$parms

# knot locations when using `x4_C`
rcs.fit.C <- rcs(data$x4_C, k_x4)
attributes(rcs.fit.C)$parms
attributes(rcs.fit.C)$parms + x_mean

# And only the `data` column is affected,
# for other columns, the horizontal change was cancelled out.
head(labelled::remove_attributes(
  rcs.fit,
  names(attributes(rcs.fit))[3:length(names(attributes(rcs.fit)))]
)) # don't print the attributes
head(labelled::remove_attributes(
  rcs.fit.C,
  names(attributes(rcs.fit.C))[3:length(names(attributes(rcs.fit.C)))]
)) # don't print the attributes
```

```{r}
# --- linear regression model on `rcs(x4,k_x4)` ---
fit <- lm(y ~ rcs(x4, k_x4), data = data)
coef(fit)

# --- linear regression model on centered variable `rcs(x4_C,k_x4)` ---
fit.C <- lm(y ~ rcs(x4_C, k_x4), data = data)
coef(fit.C)

# difference between the two intercepts
coef(fit)[1] + coef(fit)[2] * x_mean

# prediction using the centered algorithm when x = 0
paste0("Mean outcome in the dataset: ", mean(data$y))
predict(fit.C, newdata = data.frame(x4_C = 0))
```

### Instead of finding the root (see file in the "backup" folder), treat rcs as (k-1) variables

```{r}
# --- linear regression model on the original variable 'rcs(x4,k_x4)' ---
fit <- lm(y ~ rcs(x4, k_x4), data = data)
coef(fit)

# --- linear regression model on the original variable 'rcs(x4,k_x4)', but with rcs terms ---
# confirm if it's the same as above
fit <- lm(y ~ x4_rcs_1 + x4_rcs_2 + x4_rcs_3, data = data)
coef(fit)

# centering the rcs terms instead of x4
(x_mean_1 <- mean(data$x4_rcs_1))
(x_mean_2 <- mean(data$x4_rcs_2))
(x_mean_3 <- mean(data$x4_rcs_3))
data <- data %>%
  mutate(
    x4_rcs_1_C = x4_rcs_1 - x_mean_1,
    x4_rcs_2_C = x4_rcs_2 - x_mean_2,
    x4_rcs_3_C = x4_rcs_3 - x_mean_3
  )

# rcs.fit.C <- rcs(data$x4_C, k_x4)
# res <- labelled::remove_attributes(
#   rcs.fit.C,
#   names(attributes(rcs.fit.C))[3:length(names(attributes(rcs.fit.C)))]
# )
# head(res)

# head(data %>% select(x4_rcs_1_C, x4_rcs_2_C, x4_rcs_3_C))

# --- linear regression model on centered rcs terms "x4_rcs_1_C", "x4_rcs_2_C", "x4_rcs_3_C" ---
# confirm if it's the same as above
paste0("Mean outcome in the dataset: ", mean(data$y))
fit <- lm(y ~ x4_rcs_1_C + x4_rcs_2_C + x4_rcs_3_C, data = data)
coef(fit)
```

# Two predictors

## Centered algorithm for two *categorical* predictors

I will not repeat the math here, just use below code to confirm that the intercept ($\beta_0$) of the centered algorithm is still $\overline{y}$, and values of other coefficients are not affected by centering.

```{r}
# --- linear regression model on `x1` and `x2` ---"
fit <- lm(y ~ x1 + x2, data = data)
coef(fit)

# --- linear regression model on centered variable ---"
paste0("Mean outcome in the dataset: ", mean(data$y))
fit <- lm(y ~ x1_cat2_C + x2_cat2_C + x2_cat3_C, data = data)
coef(fit)
```

## Centered algorithm for a *categorical* predictor and a *continuous* predictor

I will not repeat the math here, just use below code to confirm that the intercept ($\beta_0$) of the centered algorithm is still $\overline{y}$, and values of other coefficients are not affected by centering.

```{r}
# --- linear regression model on `x2`, `x3` ---
fit <- lm(y ~ x2 + x3, data = data)
coef(fit)

# --- linear regression model on centered variable ---
paste0("Mean outcome in the dataset: ", mean(data$y))
fit <- lm(y ~ x2_cat2_C + x2_cat3_C + x3_C, data = data)
coef(fit)
```

## Centered algorithm for two *continuous* predictors without rcs

I will not repeat the math here, just use below code to confirm that the intercept ($\beta_0$) of the centered algorithm is still $\overline{y}$, and values of other coefficients are not affected by centering.

```{r}
x_mean <- mean(data$x5)
data <- data %>%
  mutate(x5_C = x5 - x_mean)

# --- linear regression model on `x3` and `x5` ---
fit <- lm(y ~ x3 + x5, data = data)
coef(fit)

# --- linear regression model on centered variable `x3_C` and `x5_C` ---
paste0("Mean outcome in the dataset: ", mean(data$y))
fit <- lm(y ~ x3_C + x5_C, data = data)
coef(fit)
```

## Centered algorithm for a *categorical* predictor and a *continuous* predictor with *rcs*

```{r}
# --- linear regression model on original variables 'x2', 'rcs(x4, k_x4)' ---
fit <- lm(y ~ x2 + rcs(x4, k_x4), data = data)
coef(fit)

# --- linear regression model on original variables 'x2', 'rcs(x4, k_x4)', but with dummy vars and rcs terms ---
# to confirm it's the same as above
fit <- lm(y ~ x2_cat2 + x2_cat3 + x4_rcs_1 + x4_rcs_2 + x4_rcs_3, data = data)
coef(fit)

# --- linear regression model on centered variables ---
paste0("Mean outcome in the dataset: ", mean(data$y))
fit <- lm(y ~ x2_cat2_C + x2_cat3_C + x4_rcs_1_C + x4_rcs_2_C + x4_rcs_3_C, data = data)
coef(fit)
```

## Centered algorithm for one *continuous* predictor without rcs and one with rcs

Note:

1.  In order to find the root, one needs to first fit a regression using the **centered** continuous variable (without rcs), and then use `root.search` to find the root.

2.  In the centered algorithm, after centering both variables, the intercept is not exactly $\overline{y}$, but the predicted value when all variables equal to 0 is equal to $\overline{y}$.

```{r}
# --- linear regression model on original variables 'x3', 'rcs(x4, k_x4)' ---
fit <- lm(y ~ x3 + rcs(x4, k_x4), data = data)
coef(fit)

# --- linear regression model on original variables 'x3', 'rcs(x4, k_x4)', but with rcs terms ---
# to confirm it's the same as above
fit <- lm(y ~ x3 + x4_rcs_1 + x4_rcs_2 + x4_rcs_3, data = data)
coef(fit)

# --- linear regression model on centered variables ---
paste0("Mean outcome in the dataset: ", mean(data$y))
fit <- lm(y ~ x3_C + x4_rcs_1_C + x4_rcs_2_C + x4_rcs_3_C, data = data)
coef(fit)
```

## Centered algorithm for two *continuous* predictors with *rcs*

```{r}
# --- linear regression model on original variables 'rcs(x4, k_x4)', 'rcs(x6, k_x6)' ---
fit <- lm(y ~ rcs(x4, k_x4) + rcs(x6, k_x6), data = data)
coef(fit)

# --- linear regression model on original variables 'rcs(x4, k_x4)', 'rcs(x6, k_x6)', but with rcs terms ---
# to confirm it's the same as above
fit <- lm(y ~ x4_rcs_1 + x4_rcs_2 + x4_rcs_3 + x6_rcs_1 + x6_rcs_2, data = data)
coef(fit)

# centering the rcs terms of x6
(x_mean_1 <- mean(data$x6_rcs_1))
(x_mean_2 <- mean(data$x6_rcs_2))
data <- data %>%
  mutate(
    x6_rcs_1_C = x6_rcs_1 - x_mean_1,
    x6_rcs_2_C = x6_rcs_2 - x_mean_2
  )

# --- linear regression model on centered variables ---
paste0("Mean outcome in the dataset: ", mean(data$y))
fit <- lm(y ~ x4_rcs_1_C + x4_rcs_2_C + x4_rcs_3_C + x6_rcs_1_C + x6_rcs_2_C, data = data)
coef(fit)
```

# Put everything together

```{r}
# original model
fit.o <- lm(y ~ x1 + x2 + x3 + rcs(x4, k_x4) + x5 + rcs(x6, k_x6), data = data)
coef(fit.o)

# centered model
paste0("Mean outcome in the dataset: ", mean(data$y))
fit.c <- lm(y ~ x1_cat2_C + x2_cat2_C + x2_cat3_C + x3_C + x4_rcs_1_C + x4_rcs_2_C + x4_rcs_3_C + x5_C + x6_rcs_1_C + x6_rcs_2_C, data = data)
coef(fit.c)
```

## test

Using the original or centered model, using the same predictors should result in the same predicted values.

```{r}
# define function for data processing - can be further generalized

# -------- dummy variables --------
step_dummy <- function(df) {
  df <- df %>%
    mutate(
      x1_cat2 = ifelse(x1 == "x1_cat2", 1, 0),
      x2_cat2 = ifelse(x2 == "x2_cat2", 1, 0),
      x2_cat3 = ifelse(x2 == "x2_cat3", 1, 0)
    )

  return(df)
}

# -------- rcs --------
rcs.fit <- rcs(data$x4, k_x4)
knots_x4 <- attributes(rcs.fit)$parms # knot locations
rcs.fit <- rcs(data$x6, k_x6)
knots_x6 <- attributes(rcs.fit)$parms # knot locations

step_rcs <- function(df, knots_x4, knots_x6) {
  k_x4 <- length(knots_x4)
  res <- get_rcs(df$x4, knots_x4)
  colnames(res) <- paste0("x4_rcs_", 1:(k_x4 - 1))
  df <- bind_cols(df, res)

  k_x6 <- length(knots_x6)
  res <- get_rcs(df$x6, knots_x6)
  colnames(res) <- paste0("x6_rcs_", 1:(k_x6 - 1))
  df <- bind_cols(df, res)

  return(df)
}

# -------- centering all variables, including the interaction terms --------
means <- c(
  mean(data$x1_cat2),
  mean(data$x2_cat2),
  mean(data$x2_cat3),
  mean(data$x3),
  mean(data$x4_rcs_1),
  mean(data$x4_rcs_2),
  mean(data$x4_rcs_3),
  mean(data$x5),
  mean(data$x6_rcs_1),
  mean(data$x6_rcs_2)
)
vars <- c(
  "x1_cat2",
  "x2_cat2",
  "x2_cat3",
  "x3",
  "x4_rcs_1",
  "x4_rcs_2",
  "x4_rcs_3",
  "x5",
  "x6_rcs_1",
  "x6_rcs_2"
)
means <- setNames(means, vars)

step_center <- function(df, means) {
  df <- df %>%
    mutate(
      x1_cat2_C = x1_cat2 - means["x1_cat2"],
      x2_cat2_C = x2_cat2 - means["x2_cat2"],
      x2_cat3_C = x2_cat3 - means["x2_cat3"],
      x3_C = x3 - means["x3"],
      x4_rcs_1_C = x4_rcs_1 - means["x4_rcs_1"],
      x4_rcs_2_C = x4_rcs_2 - means["x4_rcs_2"],
      x4_rcs_3_C = x4_rcs_3 - means["x4_rcs_3"],
      x5_C = x5 - means["x5"],
      x6_rcs_1_C = x6_rcs_1 - means["x6_rcs_1"],
      x6_rcs_2_C = x6_rcs_2 - means["x6_rcs_2"]
    )

  return(df)
}
```

```{r}
# a test subset, only selecting the original predictors
set.seed(100)
test <- sample_n(data %>% select(x1:x6, y), 100)

# process the test data
test <- test %>% 
  step_dummy() %>%
  step_rcs(knots_x4, knots_x6) %>%
  step_center(means)

# predicted values using the original model
test$pred.o <- predict(fit.o, newdata = test)

# predicted values using the centered model
test$pred.c <- predict(fit.c, newdata = test)

# compare the predicted values
test$pred.o == test$pred.c
test$pred.o-test$pred.c
```

```{r}

```